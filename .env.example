# =============================================================================
# Parakeet-RS WebRTC Transcriber - Complete Configuration
# =============================================================================
# Copy this file to .env and modify as needed
#
# BUILD COMMAND (all features):
#   cargo build --release --features "cuda,sortformer" --example webrtc_transcriber
#
# Or for TensorRT:
#   cargo build --release --features "tensorrt,sortformer" --example webrtc_transcriber

# =============================================================================
# GPU Configuration
# =============================================================================

# Enable GPU acceleration for inference
# Values: true, cuda, tensorrt, or leave empty for CPU
# Requires building with: --features cuda OR --features tensorrt
USE_GPU=cuda

# ONNX Runtime thread configuration
# For CPU-only: INTRA_THREADS=4, INTER_THREADS=1
# For GPU: INTRA_THREADS=2, INTER_THREADS=1
INTRA_THREADS=2
INTER_THREADS=1

# =============================================================================
# Model Paths
# =============================================================================

# Parakeet TDT model directory (English ASR - fast, accurate)
TDT_MODEL_PATH=./tdt

# Canary 1B model directory (Multilingual ASR: en, de, fr, es)
CANARY_MODEL_PATH=./canary

# Canary 180M Flash model directory (Multilingual ASR: en, de, fr, es - faster, smaller)
# Optional: Only needed if you want the faster Flash variant
# CANARY_FLASH_MODEL_PATH=./canary-flash

# Diarization model - SortFormer (speaker identification, up to 4 speakers)
# Requires building with: --features sortformer
DIAR_MODEL_PATH=./diar_streaming_sortformer_4spk-v2.onnx

# Voice Activity Detection model (Silero VAD)
VAD_MODEL_PATH=./silero_vad.onnx

# =============================================================================
# Server Configuration
# =============================================================================

# HTTP/WebSocket server port
PORT=80

# Maximum concurrent transcription sessions
MAX_CONCURRENT_SESSIONS=10

# Maximum threads for parallel transcription modes (parallel, pause_parallel)
MAX_PARALLEL_THREADS=8

# =============================================================================
# Media Files
# =============================================================================

# Directory for uploaded/stored audio files
MEDIA_DIR=./media

# Maximum upload file size in MB
MAX_UPLOAD_SIZE_MB=500

# =============================================================================
# Frontend
# =============================================================================

# Path to frontend static files
FRONTEND_PATH=./frontend

# =============================================================================
# WebRTC / ICE Configuration
# =============================================================================

# Public IP for WebRTC ICE candidates (required for NAT traversal)
# For cloud servers: use the server's public IP
# For Docker: use the host machine's public IP
PUBLIC_IP=37.63.112.129

# WebSocket host override (optional)
# Priority: WS_HOST > PUBLIC_IP > auto-detected
# Examples:
#   WS_HOST=ws://your-domain.com:80/ws
#   WS_HOST=wss://your-domain.com/ws  (for HTTPS)
WS_HOST=localhost

# TURN server for strict NAT traversal (optional)
# Use TURNS (TLS) with TCP transport when UDP is blocked
TURN_SERVER=turns:coturn.roomler.live:443?transport=tcp
TURN_USERNAME=hammer
TURN_PASSWORD=3BARyRrfb0711392a9dD4uzW

# Force TURN relay mode (required when UDP is blocked)
FORCE_RELAY=true

# =============================================================================
# SRT Live Streams
# =============================================================================

# IP address of the SRT encoder/source
SRT_ENCODER_IP=10.84.17.100

# JSON array of SRT channels
# Format: [{"name":"ChannelName","port":"portNumber"},...]
SRT_CHANNELS=[{"name":"ORF1","port":"24001"},{"name":"ORF2","port":"24002"},{"name":"KIDS","port":"24011"},{"name":"ORFS","port":"24004"},{"name":"ORF-B","port":"24013"},{"name":"ORF-K","port":"24019"},{"name":"ORF-NOE","port":"24012"},{"name":"ORF-OOE","port":"24014"},{"name":"ORF-S","port":"24015"},{"name":"ORF-ST","port":"24018"},{"name":"ORF-T","port":"24016"},{"name":"ORF-V","port":"24017"},{"name":"ORF-W","port":"24011"},{"name":"ORF-SI","port":"24016"}]

# SRT latency in microseconds (default: 200000 = 200ms)
SRT_LATENCY=200000

# SRT receive buffer size in bytes (default: 2097152 = 2MB)
SRT_RCVBUF=2097152

# =============================================================================
# Logging
# =============================================================================

# Rust log level: error, warn, info, debug, trace
RUST_LOG=info

# =============================================================================
# Transcription Modes (CLI flags, not env vars)
# =============================================================================
# These are set via command-line flags, not environment variables:
#   --speedy              Best balance of latency/quality (~0.3-1.5s)
#   --pause-based         Better accuracy, higher latency (~0.5-2.0s)
#   --low-latency         Fixed latency without pause detection (~3.5s)
#   --ultra-low-latency   Faster response (~2.5s)
#   --extreme-low-latency Fastest possible (~1.3s)
#   --lookahead           Best quality with future context (~1.0-3.0s)

# =============================================================================
# CUDA Library Path (REQUIRED for GPU)
# =============================================================================

# Path to ONNX Runtime CUDA provider libraries AND CUDA/cuDNN libraries
# Must include:
#   - libonnxruntime_providers_shared.so, libonnxruntime_providers_cuda.so (ONNX Runtime)
#   - libcudnn.so.9, libcublas.so, etc. (CUDA/cuDNN)
LD_LIBRARY_PATH=/home/ubuntu/parakeet-rs/target/release:/home/ubuntu/parakeet-rs/target/release/examples:/usr/local/cuda-12.8/lib:/usr/local/cuda-12.6/lib

# =============================================================================
# Quick Start Commands
# =============================================================================
#
# 1. Build with GPU + Diarization:
#    cargo build --release --features "cuda,sortformer" --example webrtc_transcriber
#
# 2. Create .env from this template:
#    cp .env.example .env
#
# 3. Run with all features (using .env):
#    set -a && source .env && set +a
#    sudo -E ./target/release/examples/webrtc_transcriber \
#      --tdt-model $TDT_MODEL_PATH \
#      --diar-model $DIAR_MODEL_PATH \
#      --vad-model $VAD_MODEL_PATH \
#      --media-dir $MEDIA_DIR \
#      --frontend $FRONTEND_PATH \
#      --port $PORT
#
# 4. Or run with inline env vars (single command):
#    sudo LD_LIBRARY_PATH=/home/ubuntu/parakeet-rs/target/release:/home/ubuntu/parakeet-rs/target/release/examples:/usr/local/cuda-12.8/lib:/usr/local/cuda-12.6/lib \
#      USE_GPU=cuda \
#      CANARY_MODEL_PATH=./canary \
#      SRT_ENCODER_IP=10.84.17.100 \
#      SRT_CHANNELS='[{"name":"ORF1","port":"24001"},{"name":"ORF2","port":"24002"}]' \
#      ./target/release/examples/webrtc_transcriber \
#        --tdt-model ./tdt \
#        --diar-model ./diar_streaming_sortformer_4spk-v2.onnx \
#        --vad-model ./silero_vad.onnx \
#        --media-dir ./media \
#        --frontend ./frontend \
#        --port 80
#
# =============================================================================
# Troubleshooting
# =============================================================================
#
# "Failed to load library libonnxruntime_providers_shared.so"
#   -> Set LD_LIBRARY_PATH to include the target/release directory
#
# "Failed to load library libonnxruntime_providers_cuda.so: libcudnn.so.9: cannot open"
#   -> Add CUDA lib paths to LD_LIBRARY_PATH: /usr/local/cuda-12.8/lib:/usr/local/cuda-12.6/lib
#   -> Verify cuDNN is installed: ls /usr/local/cuda*/lib/libcudnn.so*
#
# "USE_GPU=true but no GPU features compiled"
#   -> Rebuild with: cargo build --release --features "cuda,sortformer"
#
# "Permission denied" on port 80
#   -> Run with sudo or use a port > 1024
#
# Inference is slow (>1 second)
#   -> Verify USE_GPU=cuda is set
#   -> Check logs for "[GPU] USE_GPU=cuda, using CUDA execution provider"
#   -> If CPU fallback, check LD_LIBRARY_PATH includes all CUDA libraries
