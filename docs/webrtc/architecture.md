# Architecture Overview

> **Navigation**: [Index](./README.md) | Architecture | [API Reference](./api-reference.md) | [Latency Modes](./latency-modes.md) | [Frontend](./frontend.md) | [Deployment](./deployment.md)

## System Architecture

The parakeet-rs WebRTC transcriber is a **multi-session transcription server** that supports multiple concurrent transcription sessions with different models and media files. Each session streams audio via WebRTC and subtitles via WebSocket.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Multi-Session WebRTC Transcription Server                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                   â”‚
â”‚  Media Files                        Server                        Browser         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                       â”€â”€â”€â”€â”€â”€â”€â”€                      â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”‚
â”‚                                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚ Media   â”‚                 â”‚    Session Manager           â”‚                    â”‚
â”‚  â”‚ Manager â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚                    â”‚
â”‚  â”‚(./media)â”‚                 â”‚    â”‚ Session 1          â”‚   â”‚    WebRTC          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚    â”‚  â”œâ”€ FFmpeg Process â”‚â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  ğŸ”Š     â”‚
â”‚                              â”‚    â”‚  â”œâ”€ Opus Encoder   â”‚   â”‚    Audio           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚    â”‚  â””â”€ Transcriber    â”‚   â”‚                    â”‚
â”‚  â”‚ Model   â”‚                 â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚                    â”‚
â”‚  â”‚Registry â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚    WebSocket       â”‚
â”‚  â”‚(TDT/    â”‚                 â”‚    â”‚ Session 2 ...      â”‚â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  ğŸ“     â”‚
â”‚  â”‚ Canary) â”‚                 â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚    Subtitles       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚              â”‚              â”‚                    â”‚
â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                                             â”‚                                    â”‚
â”‚                                             â–¼                                    â”‚
â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚                              â”‚    HTTP/WS Server (Axum)     â”‚                    â”‚
â”‚                              â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚                    â”‚
â”‚                              â”‚    â”‚ /api/sessions      â”‚    â”‚                    â”‚
â”‚                              â”‚    â”‚ /api/models        â”‚    â”‚                    â”‚
â”‚                              â”‚    â”‚ /api/media         â”‚    â”‚                    â”‚
â”‚                              â”‚    â”‚ /api/modes         â”‚    â”‚                    â”‚
â”‚                              â”‚    â”‚ /ws/:session_id    â”‚    â”‚                    â”‚
â”‚                              â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚                    â”‚
â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Key Components

### 1. Session Manager

Manages multiple concurrent transcription sessions with independent models and configurations.

Each session maintains:
- `id`: Unique session identifier
- `model_id`: Selected transcription model
- `media_id`: Reference to media file
- `mode`: Latency mode (speedy, vad_speedy, etc.)
- `language`: Target language for transcription
- `state`: Current state (starting, running, completed, stopped)
- `progress_secs`: Current playback position
- `duration_secs`: Total media duration
- `client_count`: Number of connected WebRTC clients

### 2. Model Registry

Discovers and loads transcription models (TDT, Canary) and diarization models from environment variables.

**Supported Models:**

| Model | Description | Languages |
|-------|-------------|-----------|
| **TDT** | Token-and-Duration Transducer from NVIDIA NeMo | 25 languages |
| **Canary** | Multilingual transcription model | Configurable |
| **VAD+TDT** | Voice Activity Detection + TDT | 25 languages |
| **VAD+Canary** | Voice Activity Detection + Canary | Configurable |

### 3. Media Manager

Handles audio file uploads, storage, and lifecycle in the `./media` directory.

- Supports WAV and MP3 formats
- Maximum upload size: 1GB
- Automatic duration detection via ffprobe

### 4. WebRTC Server

Axum-based HTTP/WebSocket server for signaling and audio streaming.

- HTTP REST API for session/media management
- WebSocket for signaling and subtitle delivery
- WebRTC for ultra-low-latency audio (~100-400ms)

### 5. Transcription Engine

Supports multiple model types with different processing strategies:

- **TDT (Token-and-Duration Transducer)**: Word-level timestamps, fast inference
- **Canary**: Multilingual support with language-aware processing
- **VAD+TDT/Canary**: Voice Activity Detection triggered transcription

### 6. Speaker Diarization

Sortformer-based speaker identification (up to 4 speakers).

```rust
let diarizer = StreamingDiarizer::new(
    SortformerOnnxModel::from_path(&args.diar_model)?,
    DiarizationConfig {
        max_speakers: 4,
        chunk_duration_secs: 5.0,
        ..Default::default()
    },
)?;
```

---

## Multi-Session Architecture

The server supports multiple concurrent transcription sessions, each with:
- Independent model selection (TDT or Canary)
- Independent latency mode
- Independent media file
- Independent language setting
- Per-session WebRTC audio track
- Per-session subtitle broadcast channel

### Session Lifecycle

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     POST /api/sessions      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Created â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  Client  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â”‚ POST /api/sessions/:id/start
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Running â”‚â—„â”€â”€â”€ Audio streaming + Transcription
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚
     â”‚ DELETE /api/sessions/:id
     â”‚ or audio completes
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Completed â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Session States

| State | Description |
|-------|-------------|
| `starting` | Session created, waiting for start |
| `running` | Transcription in progress |
| `completed` | Audio finished, transcription complete |
| `stopped` | Manually stopped by user |

---

## Audio Pipeline

### Input Processing

Audio is processed from media files stored in the media directory. The server uses FFmpeg with real-time pacing (`-re` flag) to simulate live streaming:

```bash
ffmpeg -re -i /media/input.wav -f s16le -ar 16000 -ac 1 -loglevel error -
```

The `-re` flag ensures real-time playback speed, essential for synchronized audio and subtitle delivery.

### Ring Buffer Architecture

The transcription engine uses a ring buffer to maintain a sliding window of audio:

```rust
pub struct StreamingConfig {
    /// How much audio to buffer before processing (default: 10s)
    pub buffer_size_secs: f32,

    /// How often to process the buffer (default: 0.3s)
    pub process_interval_secs: f32,

    /// How far from buffer end to consider "confirmed" (default: 0.5s)
    pub confirm_threshold_secs: f32,

    /// Enable pause-based confirmation (default: true)
    pub pause_based_confirm: bool,

    /// Pause duration to trigger confirmation (default: 0.3s)
    pub pause_threshold_secs: f32,
}
```

The ring buffer serves multiple purposes:
- **Context for ASR**: Neural models perform better with surrounding context
- **Re-processing tolerance**: Allows correction of previous hypotheses
- **Smooth output**: Prevents jarring text changes

### Opus Encoding for WebRTC

Audio is encoded to Opus format for WebRTC transmission:

```rust
let encoder = opus::Encoder::new(
    SAMPLE_RATE as u32,  // 16000
    opus::Channels::Mono,
    opus::Application::Voip, // Optimized for speech
)?;

// Encode 20ms frames (320 samples at 16kHz)
let frame_samples = (SAMPLE_RATE * 20) / 1000; // 320
let encoded = encoder.encode(&samples, &mut opus_buffer)?;
```

Opus provides excellent compression for speech (typically 12-20 kbps) while maintaining quality.

---

## WebRTC Signaling and Media Flow

### Signaling Protocol

The server uses WebSocket (`/ws/:session_id`) for SDP and ICE candidate exchange:

```
Browser                    Server
   â”‚                          â”‚
   â”‚â”€â”€â”€â”€ WS Connect â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  /ws/{session_id}
   â”‚                          â”‚
   â”‚â—€â”€â”€ { type: "welcome",    â”‚  Session info + client ID
   â”‚      session: {...} }â”€â”€â”€â”€â”‚
   â”‚                          â”‚
   â”‚â”€â”€â”€â”€ { type: "ready" }â”€â”€â”€â–¶â”‚  Request offer
   â”‚                          â”‚
   â”‚â—€â”€â”€â”€ { type: "offer",  â”€â”€â”€â”‚  SDP offer
   â”‚       sdp: "..." }       â”‚
   â”‚                          â”‚
   â”‚â”€â”€â”€â”€ { type: "answer", â”€â”€â–¶â”‚  SDP answer
   â”‚       sdp: "..." }       â”‚
   â”‚                          â”‚
   â”‚â—€â”€â”€ { type: "ice-candidate" }  ICE candidates
   â”‚                          â”‚
   â”‚â”€â”€ { type: "ice-candidate" } â”€â”€â–¶
   â”‚                          â”‚
   â”‚â—€â•â•â• RTP Audio Stream â•â•â•â•â”‚  Opus encoded audio
   â”‚                          â”‚
   â”‚â—€â”€â”€ { type: "subtitle" }â”€â”€â”‚  Transcription segments
   â”‚                          â”‚
   â”‚â—€â”€â”€ { type: "status" }â”€â”€â”€â”€â”‚  Progress updates
   â”‚                          â”‚
   â”‚â—€â”€â”€ { type: "end" }â”€â”€â”€â”€â”€â”€â”€â”‚  Stream complete
```

### ICE and NAT Traversal

For NAT traversal, the server supports TURN/STUN:

```rust
// Server-side ICE configuration
let mut media_engine = MediaEngine::default();
media_engine.register_default_codecs()?;

let mut setting_engine = SettingEngine::default();

// Set public IP for host candidates (crucial for Docker)
if let Some(public_ip) = &args.public_ip {
    setting_engine.set_nat_1to1_ips(
        vec![public_ip.clone()],
        RTCIceCandidateType::Host
    );
}
```

---

## Data Flow Summary

1. **Client uploads media** â†’ Media Manager stores in `./media`
2. **Client creates session** â†’ Session Manager allocates resources
3. **Client starts session** â†’ FFmpeg process spawns, transcription begins
4. **Client joins via WebSocket** â†’ WebRTC peer connection established
5. **Audio flows** â†’ FFmpeg â†’ Opus Encoder â†’ WebRTC â†’ Browser
6. **Transcription flows** â†’ Transcriber â†’ Subtitle broadcast â†’ WebSocket â†’ Browser
7. **Session completes** â†’ Resources released, state updated

---

## Related Documentation

- [API Reference](./api-reference.md) - Complete REST and WebSocket API
- [Latency Modes](./latency-modes.md) - 10 transcription modes explained
- [Deployment](./deployment.md) - Configuration and Docker setup
